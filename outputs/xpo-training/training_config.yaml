model:
  model_name_or_path: google/gemma-3-270m-it
  tokenizer_name_or_path: google/gemma-3-270m-it
  torch_dtype: bfloat16
  trust_remote_code: false
  use_flash_attention: false
  quantization: nf4
  device_map: auto
  attn_implementation: null
  max_memory: null
peft:
  enabled: true
  r: 16
  lora_alpha: 32
  lora_dropout: 0.05
  target_modules:
  - q_proj
  - k_proj
  - v_proj
  - o_proj
  - gate_proj
  - up_proj
  - down_proj
  bias: none
  task_type: CAUSAL_LM
  modules_to_save: null
  use_rslora: false
  use_dora: false
data:
  dataset_name_or_path: trl-lib/ultrafeedback-prompt
  dataset_config: null
  dataset_split: train
  validation_split: null
  validation_split_percentage: 0.05
  format: prompt
  messages_column: messages
  instruction_column: instruction
  input_column: input
  output_column: output
  prompt_column: prompt
  max_length: null
  num_proc: 4
  streaming: false
  shuffle: true
  shuffle_seed: 42
training:
  output_dir: ./outputs/xpo-training
  num_train_epochs: 1
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 2
  gradient_accumulation_steps: 4
  learning_rate: 5.0e-07
  weight_decay: 0.01
  warmup_ratio: 0.1
  warmup_steps: 0
  max_grad_norm: 1.0
  lr_scheduler_type: cosine
  optim: adamw_torch
  logging_steps: 10
  save_steps: 500
  eval_steps: 500
  save_total_limit: 3
  evaluation_strategy: steps
  save_strategy: steps
  load_best_model_at_end: false
  metric_for_best_model: eval_loss
  greater_is_better: false
  fp16: true
  bf16: false
  tf32: true
  gradient_checkpointing: true
  gradient_checkpointing_kwargs:
    use_reentrant: false
  dataloader_num_workers: 0
  dataloader_pin_memory: true
  seed: 42
  report_to:
  - none
  run_name: null
  hub_model_id: null
  push_to_hub: false
  hub_private_repo: true
  resume_from_checkpoint: null
  max_steps: -1
  ddp_find_unused_parameters: false
  xpo:
    max_new_tokens: 64
    temperature: 0.9
    top_p: 1.0
    top_k: null
    beta:
    - 0.1
    alpha:
    - 1.0e-05
    loss_type: sigmoid
    disable_dropout: true
    missing_eos_penalty: null
  judge:
    model_name_or_path: Qwen/Qwen3-Reranker-4B
    torch_dtype: bfloat16
    max_length: 32768
    instruction: Given a user query, evaluate if the assistant response adequately
      answers the query
